# ğŸ¤– SmartCover: AI-Powered Cover Letter Generator with Resume & Portfolio Matching (AWS Deployable)

Generate personalized cover letters from job postings using LLaMA 3, LangChain, and ChromaDB. Automatically match your resume to job descriptions, retrieve relevant portfolio links, and deploy it all via Flask on AWS EC2.

---

## ğŸš€ Features

- ğŸ” **Job Scraper**: Scrapes job descriptions from URLs using BeautifulSoup
- ğŸ§  **LLM-Powered Extraction**: Uses Groq-hosted LLaMA 3 via LangChain to extract structured job data
- âœï¸ **Cover Letter Writer**: Generates a short, professional email-style cover letter with portfolio matching using your resume and the job info
- ğŸ”— **Portfolio Matching**: Uses ChromaDB to retrieve relevant project links based on job-required skills
- ğŸ“Š **Skill Match Score**: Calculates how closely your resume matches the job
- ğŸŒ **Modern UI**: Flask + Bootstrap frontend with clean layout
- â˜ï¸ **Deployed on AWS EC2**: Served via Gunicorn, NGINX, and secured with HTTPS using Let's Encrypt

---

## ğŸ“Œ Note on Personalization

This version of the project is personalized for the original developer (Saketh), with:
- A static resume file (`resource/my_resume.txt`)
- A static portfolio CSV (`resource/my_portfolio.csv`)

ğŸ“¦ However, this project **can be easily extended** into a multi-user platform by:

- Adding a **resume upload UI** (file input)
- Allowing users to **upload their portfolio CSV**
- Storing user data temporarily (in-memory or database)

This would allow **any user to generate their own tailored cover letter** using their own information â€” making it a fully usable public tool.

---

## ğŸ—ï¸ Tech Stack

- **Python 3.10**
- **Flask** for web backend
- **LangChain** for LLM prompt chaining
- **Groq + LLaMA 3 (70B)** for language generation
- **ChromaDB** for semantic portfolio link search
- **BeautifulSoup + requests** for scraping
- **Bootstrap 5** for frontend styling
- **Gunicorn + NGINX** for production serving
- **systemd** for auto-start service management

---
## ğŸ” Workflow Flowchart

Below is a visual representation of the process:

<p>
  <img src="images/flowchart.png" alt="SmartCover Workflow Flowchart" width="300"/>
</p>

```
## ğŸ“ Project Structure

```
smartcover/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ chains.py                    # LangChain prompts & Groq logic
â”‚   â”œâ”€â”€ portfolio.py                 # Portfolio link matching using ChromaDB
â”‚   â”œâ”€â”€ scrape_job_description.py    # Job scraping with BeautifulSoup
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html               # Flask frontend (Bootstrap UI)
â”‚   â””â”€â”€ resource/
â”‚       â”œâ”€â”€ my_resume.txt            # Your static resume
â”‚       â””â”€â”€ my_portfolio.csv         # Your portfolio links mapped to tech stack
â”œâ”€â”€ main1.py                         # Flask app entry point
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ index.png                    # UI screenshot
â”‚   â””â”€â”€ result.pdf                   # Generated result (PDF version)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore                       # Ignore venv, .env, vectorstore, etc.
â””â”€â”€ README.md                        # Project documentation


---

## âš™ï¸ Setup Instructions

### 1. Clone the Repo
```bash
git clone https://github.com/yourusername/smartcover.git
cd smartcover
```

### 2. Create Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Set Your Environment
Create a `.env` file:
```env
GROQ_API_KEY=your_groq_api_key
```

### 4. Start the App
```bash
gunicorn -w 4 -b 127.0.0.1:5000 main1:app
```

---

## ğŸŒ Deployment (AWS EC2)

- âœ… Launch Ubuntu EC2 instance
- âœ… Clone project & install dependencies
- âœ… Set `.env` and load resume/portfolio files
- âœ… Set up `systemd` to run Gunicorn
- âœ… Configure NGINX as reverse proxy

---

## ğŸ“¸ Screenshots

- ğŸ–¼ï¸ [Index Page](images/index.png)
- ğŸ§¾ [Generated Result Page](images/result.pdf)

---

## ğŸ§‘â€ğŸ’» Author
**Saketh Ram Kalavakuntla**  
Graduate in Computational Data Science @ Purdue University
